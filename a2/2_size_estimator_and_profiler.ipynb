{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML-HW-SYS/a2/blob/main/2_size_estimator_and_profiler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl1A-uzhwpt_"
      },
      "source": [
        "# **2. Model Size Estimation**\n",
        "\n",
        "It is no surprise that with such a tiny package, your Ardunio Nano 33 BLE Sense comes with limited memory and processing power. Therefore, you must be aware of the size and components of your model in order to have it run efficiently on your MCU.\n",
        "\n",
        "This notebook explores how various neural network layers affect the number of parameters, the amount memory, the number of floating point operations, and the CPU runtime of your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5kS114ooq-0"
      },
      "source": [
        "## 2.0 Setup GDrive and Git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BgbZjaQZ8niT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "052cf48c-a862-4621-b7a4-f8699ee1bf06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sH-xe50YtFNd"
      },
      "outputs": [],
      "source": [
        "# Make sure your token is stored in a txt file at the location below.\n",
        "# This way there is no risk that you will push it to your repo\n",
        "# Never share your token with anyone, it is basically your github password!\n",
        "with open('/content/drive/MyDrive/ece5545/token.txt') as f:\n",
        "    token = f.readline().strip()\n",
        "# Use another file to store your github username    \n",
        "with open('/content/drive/MyDrive/ece5545/git_username.txt') as f:\n",
        "    handle = f.readline().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rRj2U4Y3ttgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4312d033-edd6-4dd1-c0bf-9447d76d0666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/ece5545’: File exists\n",
            "/content/drive/MyDrive/ece5545\n",
            "fatal: destination path 'a2-NamanMakkar' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/ece5545/a2-NamanMakkar\n",
            "Already on 'main'\n",
            "Your branch is up to date with 'origin/main'.\n",
            "Already up to date.\n",
            "/content/drive/MyDrive/ece5545\n"
          ]
        }
      ],
      "source": [
        "# Clone your github repo\n",
        "YOUR_TOKEN = token\n",
        "YOUR_HANDLE = handle\n",
        "BRANCH = \"main\"\n",
        "\n",
        "%mkdir /content/drive/MyDrive/ece5545\n",
        "%cd /content/drive/MyDrive/ece5545\n",
        "!git clone https://{YOUR_TOKEN}@github.com/ML-HW-SYS/a2-{YOUR_HANDLE}.git\n",
        "%cd /content/drive/MyDrive/ece5545/a2-{YOUR_HANDLE}\n",
        "!git checkout {BRANCH}\n",
        "!git pull\n",
        "%cd /content/drive/MyDrive/ece5545\n",
        "\n",
        "PROJECT_ROOT = f\"/content/drive/MyDrive/ece5545/a2-{YOUR_HANDLE}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R9V2uP4YtzuR"
      },
      "outputs": [],
      "source": [
        "# This extension reloads all imports before running each cell\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIzgKXcDtFNe"
      },
      "source": [
        "### Import code dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ePTgb55gwT7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d5cbbd-1c5d-4ebb-bc12-0c9f87b70825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ece5545/a2-NamanMakkar\n",
            "constants.py  networks.py\t   quant.py\n",
            "data_proc.py  __pycache__\t   size_estimate.py\n",
            "loaders.py    quant_conversion.py  train_val_test_utils.py\n",
            "['/content', '/env/python', '/usr/lib/python39.zip', '/usr/lib/python3.9', '/usr/lib/python3.9/lib-dynload', '', '/usr/local/lib/python3.9/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.9/dist-packages/IPython/extensions', '/root/.ipython']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(PROJECT_ROOT)\n",
        "!ls {PROJECT_ROOT}/src\n",
        "print(sys.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2uKEHl-OtFNf",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a19fe253-eb42-4e0b-fc48-57edb7853f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model folders are created, \n",
            "PyTorch models will be saved in /content/drive/MyDrive/ece5545/models/torch_models, \n",
            "ONNX models will be saved in /content/drive/MyDrive/ece5545/models/onnx_models, \n",
            "TensorFlow Saved Models will be saved in /content/drive/MyDrive/ece5545/models/tf_models, \n",
            "TensorFlow Lite models will be saved in /content/drive/MyDrive/ece5545/models/tflite_models, \n",
            "TensorFlow Lite Micro models will be saved in /content/drive/MyDrive/ece5545/models/micro_models.\n",
            "Imported code dependencies\n"
          ]
        }
      ],
      "source": [
        "import sys,os\n",
        "\n",
        "# Adding assignment 2 to the system path\n",
        "# Make sure this matches your git directory\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nnt\n",
        "import src.data_proc as data_proc\n",
        "from src.constants import *\n",
        "import numpy as np\n",
        "\n",
        "print(\"Imported code dependencies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTKG-QIfwcN9"
      },
      "source": [
        "## 2.2 Define the Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RRSPvUTyIQ2"
      },
      "source": [
        "### Create the model\n",
        "Our TinyConv model currently consists of 7 layers:\n",
        "\n",
        "\n",
        "1. [Reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html)\n",
        "2. [Conv2D](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n",
        "3. [Relu](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU) \n",
        "4. [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout) \n",
        "5. Reshape\n",
        "6. [Fully Connected (Linear)](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
        "7. [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax)\n",
        "\n",
        "\n",
        "Please refer to `<github_dir>/src/networks.py` for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jU2ryBhlwcN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691cace8-1975-4a89-b63f-510d3e025629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda to run the training scrpit.\n"
          ]
        }
      ],
      "source": [
        "# Define device\n",
        "from src.networks import TinyConv\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using {device} to run the training scrpit.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLwsdGt_liKa"
      },
      "source": [
        "### Create data_proc.AudioProcessor() object for data preprocessing\n",
        "When an AudioProcessor instance is created: \n",
        "\n",
        "1. Download speech_command dataset from DATA_URL (defined in constants.py) to data_dir (default: '/content/gdrive/MyDrive/ece5545/data')\n",
        "default dataset url: 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "\n",
        "2. Determine classes and their numerical indices for training and testing based on WANTED_WORDS \n",
        "(defined in constants.py): \n",
        "eg. if WANTED_WORDS is ['yes', 'no'], model will be trained to identify \"yes\" and \"no\" as yes and no, \n",
        "other words as unkown, and background noises as silence\n",
        "\n",
        "3. Determine and save the settings for data processing feature generator based on relavent constants \n",
        "in constants.py\n",
        "\n",
        "4. Determine which audio files in the dataset are for testing, training, or validating using hash method\n",
        "\n",
        "5. Prepare and save background noise data using the background noise data inside dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HLxcu6BB_wxg"
      },
      "outputs": [],
      "source": [
        "# Create audio processor (this takes some time the first time)\n",
        "# And continues to run for a bit after reaching 100% while it's extracting files\n",
        "audio_processor = data_proc.AudioProcessor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_RJ8a0otJEJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c164591-ca3b-4d2b-8433-f3effbb558ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyConv(\n",
              "  (conv_reshape): Reshape(output_shape=(-1, 1, 49, 40))\n",
              "  (conv): Conv2d(1, 8, kernel_size=(10, 8), stride=(2, 2), padding=(5, 3))\n",
              "  (relu): ReLU()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_reshape): Reshape(output_shape=(-1, 4000))\n",
              "  (fc): Linear(in_features=4000, out_features=4, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Create model\n",
        "model_fp32 = TinyConv(audio_processor.model_settings)\n",
        "model_fp32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcB7Ri8ewcOA"
      },
      "source": [
        "## 2.3 Model Estimates\n",
        "Run the next few cells to see how each layer impacts memory and runtime of the below TinyConv neural network model. Then experiment with reshaping it to see how adding or removing layers alters the metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEnwhmThwcOC"
      },
      "source": [
        "### Memory Utilization\n",
        "\n",
        "There are two important forms of memory that we care about for MCUs: **flash memory** and **random access memory (RAM)**. Flash is **non-volatile** aka persistent storage memory; its data is saved when powered off. This is where your model's weights and code live, thus they must be able to fit within the capacity of your MCU's flash memory (1MB). On the other hand, RAM is **volatile** or non-persistent memory, thus it is used for temporary storage like input buffers and intermediate tensors. Together, they cannot exceed the size of your RAM storage (256KB).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9Gv8LHUtFNi"
      },
      "source": [
        "### TODO 1: Implement the `count_trainable_parameters` function in `src/size_estimate.py` to compute model size and get an estimate of the flash usage of this model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--J01LrjtFNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862f6661-7798-4df7-9456-02f30b824584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of trainable parameters:  0.016652 M\n"
          ]
        }
      ],
      "source": [
        "# Sends model weights to the GPU if tensors are on GPU\n",
        "if torch.cuda.is_available():\n",
        "    model_fp32.cuda()\n",
        "\n",
        "from src.size_estimate import count_trainable_parameters\n",
        "num_params = count_trainable_parameters(model_fp32)\n",
        "print(\"Total number of trainable parameters: \", num_params / float(1e6), \"M\") # Should be about 0.016652 M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZaeZSLrtFNi"
      },
      "source": [
        "### TODO 2: Implement the `compute_forward_size` function in `src/size_estimate.py` to compute the memory needed for a forward pass. This is how much RAM you will be using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLJ-LBfo56IL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0880b5ef-85a9-4905-bdeb-1b8b1119e694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward memory:  0.007856 M\n"
          ]
        }
      ],
      "source": [
        "# Sends model weights to the GPU if tensors are on GPU\n",
        "if torch.cuda.is_available():\n",
        "    model_fp32.cuda()\n",
        "\n",
        "from src.size_estimate import compute_forward_memory\n",
        "frd_memory = compute_forward_memory(\n",
        "    model_fp32,\n",
        "    (1, model_fp32.model_settings['fingerprint_width'], model_fp32.model_settings['spectrogram_length']),\n",
        "    device\n",
        ")\n",
        "print(\"Forward memory: \", frd_memory / float(1e6), \"M\") # Should be about 0.03462 M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkjm2OlXgArr"
      },
      "source": [
        "As you can see above, the number of parameters in a neural network can add up fast which is a concern when dealing with a small amount of RAM. With the TinyConv neural network only consuming 0.21MB out of 1MB, our model will easily fit within flash memory. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4qkx4L8hCBD"
      },
      "source": [
        "### Number of Operations\n",
        "\n",
        "### TODO 3: Implement the `flop` function in `src/size_estimate.py` to count the total FLOPS in a forward pass with batch size = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb5ugZA3wcOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e728166b-8fcc-4a93-9225-7538f30b22e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of floating operations: 676004\n",
            "Number of FLOPs by layer and parameters:\n",
            "Conv:  {Conv2d(1, 8, kernel_size=(10, 8), stride=(2, 2), padding=(5, 3)): 644000}\n",
            "FC:    {Linear(in_features=4000, out_features=4, bias=True): 32004}\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "from src.size_estimate import flop\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model_fp32.cuda()\n",
        "\n",
        "# The total number of floating point operations \n",
        "flop_by_layers = flop(\n",
        "    model=model_fp32, \n",
        "    input_shape=(\n",
        "        1, \n",
        "        model_fp32.model_settings['fingerprint_width'], \n",
        "        model_fp32.model_settings['spectrogram_length']\n",
        "    ), \n",
        "    device=device)\n",
        "total_param_flops = sum([sum(val.values()) for val in flop_by_layers.values()])\n",
        "\n",
        "\n",
        "print(f'total number of floating operations: {total_param_flops}')  # total number of floating operations: 340004\n",
        "print('Number of FLOPs by layer and parameters:') \n",
        "print(\"Conv: \", flop_by_layers['conv'])  # {'bias': 4000, 'weight': 320000}\n",
        "print(\"FC:   \", flop_by_layers['fc'])  # {'bias': 4, 'weight': 16000}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbAbpr4Mj12i"
      },
      "source": [
        "### CPU runtime\n",
        "\n",
        "### TODO 4: Measure the server/desktop CPU runtime to compare to the MCU runtime later in this assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zMcAtzKHwcOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9d5d026-f6cf-44b7-a280-2a46e45fa732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  Total KFLOPs  \n",
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "             model_inference         7.29%       9.300ms        94.76%     120.824ms     120.824ms             1            --  \n",
            "                aten::conv2d         0.17%     220.000us        59.18%      75.456ms      75.456ms             1       640.000  \n",
            "           aten::convolution         2.29%       2.919ms        59.01%      75.236ms      75.236ms             1            --  \n",
            "          aten::_convolution         1.93%       2.463ms        56.72%      72.317ms      72.317ms             1            --  \n",
            "    aten::mkldnn_convolution        54.76%      69.819ms        54.79%      69.854ms      69.854ms             1            --  \n",
            "                aten::linear         1.55%       1.977ms        14.86%      18.949ms      18.949ms             1            --  \n",
            "                 aten::addmm        11.53%      14.702ms        13.25%      16.900ms      16.900ms             1        32.000  \n",
            "                  aten::relu         0.18%     233.000us         5.69%       7.254ms       7.254ms             1            --  \n",
            "             aten::clamp_min         5.51%       7.021ms         5.51%       7.021ms       7.021ms             1            --  \n",
            "                 aten::zeros         5.14%       6.554ms         5.24%       6.677ms       6.677ms             1            --  \n",
            "               aten::softmax         2.21%       2.815ms         5.04%       6.429ms       6.429ms             1            --  \n",
            "              aten::_softmax         2.83%       3.614ms         2.83%       3.614ms       3.614ms             1            --  \n",
            "               aten::reshape         2.25%       2.872ms         2.69%       3.429ms       1.714ms             2            --  \n",
            "                 aten::copy_         1.70%       2.167ms         1.70%       2.167ms       2.167ms             1            --  \n",
            "        aten::_reshape_alias         0.44%     557.000us         0.44%     557.000us     278.500us             2            --  \n",
            "                 aten::zero_         0.09%     111.000us         0.09%     111.000us     111.000us             1            --  \n",
            "                     aten::t         0.05%      58.000us         0.06%      72.000us      72.000us             1            --  \n",
            "                 aten::empty         0.03%      38.000us         0.03%      38.000us       9.500us             4            --  \n",
            "                aten::expand         0.02%      25.000us         0.02%      27.000us      27.000us             1            --  \n",
            "           aten::as_strided_         0.01%      16.000us         0.01%      16.000us      16.000us             1            --  \n",
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 127.501ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_fp32.cpu()\n",
        "model_fp32.eval()\n",
        "inputs = torch.rand([1,1960]).cpu()\n",
        "\n",
        "# Run a profiler to see the cpu time for inference \n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, with_flops=True, with_stack=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        model_fp32(inputs)\n",
        "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_fp32.cuda()\n",
        "model_fp32.eval()\n",
        "inputs = torch.rand([1,1960]).cuda()\n",
        "\n",
        "# Run a profiler to see the cpu time for inference \n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "with profile(activities=[ProfilerActivity.CUDA], record_shapes=True, with_flops=True, with_stack=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        model_fp32(inputs)\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t06jv8iKFbh1",
        "outputId": "d47c91a3-82e6-4012-8108-c47b014fe422"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "void implicit_convolve_sgemm<float, float, 128, 5, 5...         0.00%       0.000us         0.00%       0.000us       0.000us      18.000us        30.00%      18.000us      18.000us             1  \n",
            "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us      17.000us        28.33%      17.000us       4.250us             4  \n",
            "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       6.000us        10.00%       6.000us       6.000us             1  \n",
            "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us       6.000us        10.00%       6.000us       6.000us             1  \n",
            "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us       5.000us         8.33%       5.000us       5.000us             1  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us         6.67%       4.000us       4.000us             1  \n",
            "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us         6.67%       4.000us       4.000us             1  \n",
            "                                     cudaGetDeviceCount         0.00%       1.000us         0.00%       1.000us       1.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                   cudaDriverGetVersion         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                 cudaDeviceGetAttribute         0.00%       1.000us         0.00%       1.000us       0.031us       0.000us         0.00%       0.000us       0.000us            32  \n",
            "                                cudaGetDeviceProperties         0.00%     107.000us         0.00%     107.000us     107.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                              cudaStreamCreateWithFlags         7.58%     417.887ms         7.58%     417.887ms      52.236ms       0.000us         0.00%       0.000us       0.000us             8  \n",
            "                       cudaDeviceGetStreamPriorityRange         0.00%       1.000us         0.00%       1.000us       0.500us       0.000us         0.00%       0.000us       0.000us             2  \n",
            "                           cudaStreamCreateWithPriority         0.00%     146.000us         0.00%     146.000us      36.500us       0.000us         0.00%       0.000us       0.000us             4  \n",
            "                                             cudaMalloc         0.01%     550.000us         0.01%     550.000us      55.000us       0.000us         0.00%       0.000us       0.000us            10  \n",
            "                                        cudaMemsetAsync         0.00%     101.000us         0.00%     101.000us      25.250us       0.000us         0.00%       0.000us       0.000us             4  \n",
            "                                          cudaHostAlloc         0.02%     962.000us         0.02%     962.000us     962.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                               cudaHostGetDevicePointer         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                               cudaFree        48.52%        2.675s        48.52%        2.675s     668.692ms       0.000us         0.00%       0.000us       0.000us             4  \n",
            "                                   cudaGetSymbolAddress         0.00%       2.000us         0.00%       2.000us       2.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 5.513s\n",
            "Self CUDA time total: 60.000us\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eidFHTAqCWOO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "92bf126df007708fd70c442c808ee74575bedf7ea6317e0b182c3af0184af25d"
    },
    "kernelspec": {
      "display_name": "ece5545",
      "language": "python",
      "name": "ece5545"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}